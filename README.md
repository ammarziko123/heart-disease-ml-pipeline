# Heart Disease UCI â€” Endâ€‘toâ€‘End ML Pipeline

A complete, reproducible machineâ€‘learning pipeline to analyze, predict, and visualize heart disease risk.

## âœ¨ Whatâ€™s inside
- **Data preprocessing & cleaning** (missing values, encoding, scaling)
- **Feature selection**: Chiâ€‘Square, ANOVA (f_classif), and **RFE**
- **Dimensionality reduction**: **PCA**
- **Supervised models**: Logistic Regression, Decision Tree, Random Forest, SVM
- **Unsupervised**: Kâ€‘Means & Hierarchical (Agglomerative) clustering
- **Hyperparameter tuning**: GridSearchCV & RandomizedSearchCV
- **Evaluation**: accuracy, precision/recall/F1, ROCâ€‘AUC, confusion matrix, learning curves
- **Deployment**: Streamlit app (bonus), optional Ngrok tunnel (bonus)
- **Reproducible outputs**: trained pipeline + reports in `./artifacts`

> **Dataset**: Uses the commonly used, preâ€‘processed *Heart Disease* CSV (Kaggle/various mirrors) with columns like `age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, target` where `target` is 1 for disease, 0 otherwise.

## ğŸ—‚ Project structure
```
heart-disease-ml-pipeline/
â”œâ”€ data/
â”‚  â””â”€ heart.csv                # Place dataset here (or run the downloader)
â”œâ”€ src/
â”‚  â”œâ”€ preprocess.py            # Cleaning, encoding, scaling, PCA, feature selection
â”‚  â”œâ”€ models.py                # Model spaces, search grids, training helpers
â”‚  â”œâ”€ unsupervised.py          # KMeans & Agglomerative + plots & metrics
â”‚  â”œâ”€ utils.py                 # Utilities (plots, reports, I/O)
â”œâ”€ artifacts/
â”‚  â”œâ”€ best_supervised_pipeline.joblib
â”‚  â”œâ”€ feature_info.json
â”‚  â””â”€ reports/                 # Auto-generated metrics and plots
â”œâ”€ train.py                    # Orchestrates supervised training + tuning
â”œâ”€ cluster.py                  # Runs unsupervised analysis & figures
â”œâ”€ app.py                      # Streamlit UI for live inference
â”œâ”€ download_data.py            # Convenience script to fetch heart.csv
â”œâ”€ requirements.txt
â””â”€ README.md
```

## â¬‡ï¸ Get the data
Option A â€” Put `heart.csv` into `./data` yourself.

Option B â€” Let the script try common mirrors:
```bash
python download_data.py
```

## â–¶ï¸ Train supervised models
```bash
pip install -r requirements.txt

# Train + tune (saves best pipeline to ./artifacts)
python train.py
```

Outputs in `./artifacts`:
- `best_supervised_pipeline.joblib` â€” the **fitted Pipeline** (preprocessing + model)
- `feature_info.json` â€” column metadata (dtypes, order, required inputs)
- `reports/` â€” classification report, confusion matrix, ROC curve, learning curves

## ğŸ” Unsupervised exploration
```bash
python cluster.py
```
Generates cluster labels, silhouette score, and 2D PCA plots into `./artifacts/reports`.

## ğŸ–¥ Streamlit app (bonus)
```bash
streamlit run app.py
```
- Loads `artifacts/best_supervised_pipeline.joblib` and `feature_info.json`
- Web form for feature inputs
- Predicts probability & class, shows interpretation hints

### ğŸŒ Ngrok (bonus)
1) Install ngrok and get a token from ngrok.com.
2) In a second terminal:
```bash
ngrok http 8501
```
3) Share the forwarding URL.

## ğŸ§ª Notes
- Reproducibility: random seeds are fixed.
- The Streamlit app hides technical columns (like oneâ€‘hot vectors); it asks for humanâ€‘readable inputs and maps them internally.
- The training code uses sensible defaults but is easy to extend (e.g., XGBoost, LightGBM).

## ğŸ“¦ Requirements
See `requirements.txt`.
